<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>r on Joshua M. Rosenberg, Ph.D.</title>
    <link>/categories/r/</link>
    <description>Recent content in r on Joshua M. Rosenberg, Ph.D.</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 22 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Getting a grip on using BibTeX with R Markdown documents</title>
      <link>/post/2019/12/22/using-bibtex-with-r-markdown/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/12/22/using-bibtex-with-r-markdown/</guid>
      <description>I’m just beginning to learn BibTeX, which is, according to Wikipedia (???), “reference management software for formatting lists of references”.
It is useful for publishing formats such as R Markdown documents and books created with bookdown, like the Data Science in Education Using R book.
I’m basically using this post to document what I’m learning.
Adding references I started with references, which are saved in plain text documents ending in .</description>
    </item>
    
    <item>
      <title>The first, rough (rough!) draft of the (web-based) version of Data Science in Education Using R is available</title>
      <link>/post/2019/12/14/the-first-rough-draft-of-the-web-based-version-of-data-science-in-education-using-r-is-available/</link>
      <pubDate>Sat, 14 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/12/14/the-first-rough-draft-of-the-web-based-version-of-data-science-in-education-using-r-is-available/</guid>
      <description>The first, rough draft of the (web-based version) of the Data Science in Education Using R book written by Emily Bovee, Ryan Estrellado, Jesse Motstipak, me, and Isabella Velásquez is available from this URL: http://www.datascienceineducation.com/.
I mentioned this in an earlier post introducing the book: the publisher, Routledge, allowed us to stipulate that we retain the copyright for the web-based version, even after the book is published in print (and e-book) format.</description>
    </item>
    
    <item>
      <title>A tiny (Shiny) app: How Many (MCMC) Cores?</title>
      <link>/post/2019/12/13/a-tiny-shiny-how-many-mcmc-cores/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/12/13/a-tiny-shiny-how-many-mcmc-cores/</guid>
      <description>Here is a tiny (Shiny) application to help with selecting the number of cores in the context of MCMC estimation.
How many cores? is here: https://jmichaelrosenberg.shinyapps.io/how-many-cores/
Here is a screenshot:
The source is here: https://github.com/jrosen48/how-many-cores</description>
    </item>
    
    <item>
      <title>Using R/R Studio from a (Pixel Slate) Chromebook</title>
      <link>/post/2019/12/10/using-r-r-studio-from-a-pixel-slate-chromebook/</link>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/12/10/using-r-r-studio-from-a-pixel-slate-chromebook/</guid>
      <description>I&amp;rsquo;ve had a Pixel Slate Chromebook for a little while, and noticed in a recent update that it was possible to install Linux.
This was pretty easy to do (see here). After doing it (it took a few minutes to download Linux, but was otherwise super straightforward), I opened up the command line interface and saw this:
  There are a few things I know how to do through a command line interface; one is to list all of the files and directories available, so I ran the following:</description>
    </item>
    
    <item>
      <title>Writing a CV in markdown so it can be rendered as HTML, a PDF, and a Word document</title>
      <link>/posts/creating-a-cv-while-using-blogdown-pdf-html-and-word-doc-oh-my/</link>
      <pubDate>Thu, 25 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/creating-a-cv-while-using-blogdown-pdf-html-and-word-doc-oh-my/</guid>
      <description>My goal: To write a CV in one format and in a way so that it can be rendered in multiple formats I have been trying to write my CV once and have it be displayed/rendered as HTML (for this website), a PDF (for sharing), and a Word document (also for sharing - as some folks require it to be in this format).
I have something that seems to work and so thought I would share what I did as a first attempt.</description>
    </item>
    
    <item>
      <title>Using R Markdown and papaja with a LaTeX class to submit a conference proposal: A few notes</title>
      <link>/posts/using-r-markdown-with-a-latex-class-a-few-notes/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/using-r-markdown-with-a-latex-class-a-few-notes/</guid>
      <description>With my colleagues Emily Bovee (who had the idea for the project and is the first author) and John Ranellucci, I worked on a conference proposal for a conference that provided either a Word Document template or a LaTeX class. Since I had just used (that’s a generous way to put it - I really had struggled through it) LaTeX, I proposed that we see whether we could (continue to) use R and R Markdown, which we used to analyze the data, to prepare the manuscript.</description>
    </item>
    
    <item>
      <title>A quick introduction and tutorial on a cool social network analysis model for influence</title>
      <link>/posts/social-network-analysis-model-for-influence/</link>
      <pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/social-network-analysis-model-for-influence/</guid>
      <description>Social network is increasingly common in education (Sweet, 2016). Very often, social network analysis is used to create sociograms, or depictions of a network. I did this in a paper I recently co-authored (here is a pre-print), we worked hard to create this figure:

Such figures (we thought–and I think) look nice and they are often useful for understanding the characteristics of a network and the relations that are part of it.</description>
    </item>
    
    <item>
      <title>In what months are educational psychology jobs posted? An update!</title>
      <link>/posts/in-what-months-are-educational-psychology-jobs-posted-an-update/</link>
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/in-what-months-are-educational-psychology-jobs-posted-an-update/</guid>
      <description>In August of last year, I wrote a post that involved looking at when educational psychology jobs (mostly, but not only, academic jobs or those related to research and teaching) were posted to the excellent Ed Psych Jobs website. As that post made (in part) the (anticipated) point that most jobs were posted in September (in which 16% of jobs were posted there) and October (14%) (followed by August, with 11%), I thought that it may be helpful to see how accurate that was over the past year.</description>
    </item>
    
    <item>
      <title>Learning R (for data analysis and data science): Where to start</title>
      <link>/posts/learning-r-where-to-start-crowdsourced-recommendations/</link>
      <pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/learning-r-where-to-start-crowdsourced-recommendations/</guid>
      <description>A friend of a friend (also in educational research) posted that he was interested in learning R.
I had a couple of ideas but knew that others might have better ideas. So, I posted (on Twitter) looking for recommendations and received some excellent talks, links, and other resources.
Here they are, in Tweet form (you may have to click on the links in tweets to see some of the resources and content)</description>
    </item>
    
    <item>
      <title>An R package for sensitivity analysis (konfound)</title>
      <link>/posts/an-r-package-for-sensitivity-analysis-konfound/</link>
      <pubDate>Thu, 12 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/an-r-package-for-sensitivity-analysis-konfound/</guid>
      <description>knitr::opts_chunk$set( comment = &amp;quot;#&amp;gt;&amp;quot;, collapse = TRUE ) With Ran Xu and Ken Frank, I have worked on a Shiny interactive web application for sensitivity analysis as well as an R package for carrying out sensitivity analysis using R.
That R package is now available on CRAN! A link to the CRAN page for it is here and the website for the package is here.
Here is the description:</description>
    </item>
    
    <item>
      <title>Explorations in Markov Chain Monte Carlo - comparing results from MCMCglmm and lme4</title>
      <link>/posts/explorations-in-markov-chain-monte-carlo-mcmc/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/explorations-in-markov-chain-monte-carlo-mcmc/</guid>
      <description>Note that due to version-related issues the code in this post is displayed but is not excecuted/run. If you run this code, parts may not work (depending upon the version of the packages used and, potentially, other factors).
Introduction I’ve been interested in Markov Chain Monte Carlo (MCMC) for a little while, in part because of a paper by Tom Houslay and Alastair Wilson (2017) that shows how using output from models the way I have been can lead to results that overstate the impact of effects.</description>
    </item>
    
    <item>
      <title>Finding the top rail-trails in each state using mixed effects models</title>
      <link>/posts/find-the-top-rail-trails-in-each-state/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/find-the-top-rail-trails-in-each-state/</guid>
      <description>Outside of education, one of my interests is cycling, and one of my favorite ways to cycle is on rail-trails, pathways and greenways that are converted from former railroad tracks.
In a side-project (and because the data source can be used for teaching and learning about complex, nested data), I collected information from the TrailLink website. I’ve blogged about this data here and here to find out what the best rail-trails in Michigan are and to find out what the characteristics of the best rail-trails are, respectively.</description>
    </item>
    
    <item>
      <title>Introducing tidyLPA (an R package for carrying out Latent Profile Analysis)</title>
      <link>/posts/introducing-tidylpa-an-r-package-for-carrying-out-latent-profile-analysis/</link>
      <pubDate>Wed, 14 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/introducing-tidylpa-an-r-package-for-carrying-out-latent-profile-analysis/</guid>
      <description>I’m excited to introduce tidyLPA, an R package for carrying out Latent Profile Analysis (LPA). This is the result of a collaborative project with Jennifer Schmidt, Patrick Beymer, and Rebecca Steingut, and is the result of a long period of learning about cluster analysis (see here) and, recently, model-based cluster analysis. Here, I introduce and describe LPA as a particular type of model-based cluster analysis.
Note: This post has been updated to reflect the latest version of tidyLPA</description>
    </item>
    
    <item>
      <title>A Shiny interactive web application to quantify how robust inferences are to potential sources of bias (sensitivity analysis)</title>
      <link>/posts/a-shiny-interactive-web-application-to-quantify-how-robust-inferences-are-to-potential-sources-of-bias-sensitivity-analysis/</link>
      <pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/a-shiny-interactive-web-application-to-quantify-how-robust-inferences-are-to-potential-sources-of-bias-sensitivity-analysis/</guid>
      <description>As part of a revise and resubmit decision for a paper (that was just accepted to the Journal of Youth and Adolescence (pre-print here)), we (Patrick Beymer, Jennifer Schmidt, and I) were asked by the editor to carry out sensitivity analysis for findings. Our understanding was that, in this context, sensitivity analysis means one of two things - or both:
 How results hold up under different specfications of an analyses How much bias would have to be present to invalidate an inference  Over the past year or so, I had learned about sensitivity analysis from a class and then work with Ken Frank.</description>
    </item>
    
    <item>
      <title>Modifying an R function to iterate (using purrr) and use non-standard evaluation (using rlang)</title>
      <link>/posts/modifying-an-r-function-to-use-non-standard-evaluation/</link>
      <pubDate>Sun, 17 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/modifying-an-r-function-to-use-non-standard-evaluation/</guid>
      <description>Note that due to version-related issues the code in this post is displayed but is not excecuted/run.
Background Research in classrooms and schools can be complex because of all of the factors that matter. A question that often comes up when we say that we observed some pattern in data is, but did you control for X?
In the context of working on an approach to find out how impactful an omitted variable would need to be to invalidate an inference, we had to modify a function that worked for a single sensitivity analysis to work for many and to be easier to use.</description>
    </item>
    
    <item>
      <title>Two data packages: Rail-trails and an assessment of student achievement</title>
      <link>/posts/two-open-datasets-rail-trails-and-an-assessment-of-student-achievement/</link>
      <pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/two-open-datasets-rail-trails-and-an-assessment-of-student-achievement/</guid>
      <description>Because of interest and the need for better examples (for teaching and for use in tools under-development), such as prcr and tidyLPA, I worked to create two data packages, data easily available through an R package.
A benefit of the data being in an R package is that it is even easier to access than other formats (in R): Just load the name of the package and type the name of the data frame, or, if the data is included as built-in data in another package (one that is loaded), just type the name of the data frame.</description>
    </item>
    
    <item>
      <title>Getting started with &#39;open science&#39; through blogging</title>
      <link>/posts/getting-started-with-open-science-through-blogging/</link>
      <pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/getting-started-with-open-science-through-blogging/</guid>
      <description>Through a few different projects and people (such as SIPS and rOpenSci and conversations with friends / colleagues both online and offline), I have been exposed to the idea of open science.
I’m actually going to punt for the moment. Here’s a definition that sounds about right to me:
 Open science is the movement to make scientific research, data and dissemination accessible to all levels of an inquiring society, amateur or professional.</description>
    </item>
    
    <item>
      <title>Using MPlus from R with MPlusAutomation</title>
      <link>/posts/using-mplus-from-r/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/using-mplus-from-r/</guid>
      <description>Note that due to version-related issues the code in this post is displayed but is not excecuted/run.
According to the MPlus website, the R package MPlusAutomation serves three purposes:
Creating related groups of models Running batches Extracting and tabulating model parameters and test statistics.  Because modeling involves comparing related models, (partially) automating these is compelling. It can make it easier to use model results in subsequent analyses and can cut down on copy and pasting output or results between programs.</description>
    </item>
    
    <item>
      <title>A first pass at Latent Profile Analysis using MCLUST (in R)</title>
      <link>/posts/lpa-in-r-using-mclust/</link>
      <pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/lpa-in-r-using-mclust/</guid>
      <description>Note - an R package for carrying out Latent Profile Analysis (LPA): tidyLPA Since writing this post, I have worked with colleagues to release an R package to carry out the analysis. The package is tidyLPA and is described in the following sections, through the “Original Post” header below which is the beginning of the original post.
Introduction Latent Profile Analysis (LPA) is a statistical modeling approach for estimating distinct profiles, or groups, of variables.</description>
    </item>
    
    <item>
      <title>Comparing estimates and their standard errors from mixed effects and linear models</title>
      <link>/posts/comparing-mixed-effects-and-linear-models/</link>
      <pubDate>Tue, 15 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/comparing-mixed-effects-and-linear-models/</guid>
      <description>Some background One reason to use mixed effects models is that they help to account for data with a complex structure, such as multiple responses (to questions, for example) from the same people, students grouped into classes, and measures collected over time. Often, the way they account for these complex structures is in terms of reducing their bias, which has to do with when a model comes up with an estimate that is off - too large, too small, or maybe too certain (or uncertain) relative to the true value of the thing that is estimated.</description>
    </item>
    
    <item>
      <title>In what months are educational psychology jobs posted?</title>
      <link>/posts/when-are-ed-psych-jobs-posted/</link>
      <pubDate>Tue, 15 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/when-are-ed-psych-jobs-posted/</guid>
      <description>Division 15 of the American Psychological Association sponsors the Ed Psych Jobs website, which is an excellent resource for Ed Psych job seekers. I thought it would possibly be helpful to see when jobs were posted in the past in order to have a better idea about when jobs may be posted this year.
Ed Psych Jobs, Robots (.txt), and paths_allowed, oh my As this project involves a bit of web-scraping, I first checked the robots.</description>
    </item>
    
    <item>
      <title>Using characteristics of rail-trails to predict how they are rated</title>
      <link>/posts/characteristics-of-rail-trails/</link>
      <pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/characteristics-of-rail-trails/</guid>
      <description>Catching up I wrote a blog post (one that, to be honest, I liked a lot) on what the best rail-trails are in Michigan (here). A friend and colleague at MSU, Andy, noticed that paved trails seemed to be rated higher, and this as well as my cfriend and colleague Kristy’s comment about how we can use the output of the the previous post sparked my curiosity in trying to figure out what characteristics predict how highly (or not highly) rated trails are.</description>
    </item>
    
    <item>
      <title>What are the best rail-trails in Michigan?</title>
      <link>/posts/michigan-rail-trails-and-pathways-through-data/</link>
      <pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/michigan-rail-trails-and-pathways-through-data/</guid>
      <description>Background I was curious about what rail trails were the best in Michigan, and so to figure out an answer, I checked out the TrailLink website, sponsored by the Rails-to-Trails Conservancy. I had just purchased a copy of their book Rail-Trails Michigan and Wisconsin, and wanted to see whether I could learn more from the website.
To start, I checked whether they had a way to access the reviews on the site through an API.</description>
    </item>
    
    <item>
      <title>How many groups of Star Wars characters are there? R-squared and cross-validation approaches</title>
      <link>/posts/how-many-groups-of-star-wars-characters-are-there-r-squared-and-cross-validation-approaches/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/how-many-groups-of-star-wars-characters-are-there-r-squared-and-cross-validation-approaches/</guid>
      <description>Background How many groups, or types, of Star Wars characters are there? I’ve been wanting to use the starwars dataset built-in to the dplyr package, and at the same time, have been working hard on an R package to carry out an analysis suited to doing this. Part of the challenge of using the approach in this R package is determining how groups groups there are.
Many approaches (Latent Profile Analysis, for example) use Maximum Likelihood estimation (while the approach I’ve developed uses a two-step cluster analysis based around the geometric (and algebraic) idea of “distance”, or how close (similar) observations are).</description>
    </item>
    
    <item>
      <title>prcr update</title>
      <link>/posts/2017-05-17-prcr-update/</link>
      <pubDate>Wed, 17 May 2017 13:48:00 +0500</pubDate>
      
      <guid>/posts/2017-05-17-prcr-update/</guid>
      <description>The R package for person-oriented analysis (prcr) is updated (it’s now version 0.1.4).
In particular, it was not clear how to use the profile assignments (i.e., what cluster each response is in) in subsequent analyses. So, the update now returns two different representations of the profile assignments, or which profile is associated with each observation: a) the original “data.frame” with the profile assignment added in one variable and b) the original “data.</description>
    </item>
    
    <item>
      <title>What the world is data science education? Looking back on #dsetonline</title>
      <link>/posts/what-the-world-is-data-science-education-looking-back-on-dsetonline/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/what-the-world-is-data-science-education-looking-back-on-dsetonline/</guid>
      <description>DSET A few weeks ago, I was fortunate to attend the Data Science Educational Technology (DSET) conference.
The goal for the conference was to kickstart data science education and to explore an educational technology, Concord Consortium’s Common Online Data Analysis Platform. What’s the big idea about data science education? To me, it’s a recognition that data is power, and that through asking questions, collecting and creating data, analyzing and modeling data, and making sense of and communicating what is going on can be empowering to learners.</description>
    </item>
    
    <item>
      <title>The Internet Archive&#39;s Television News Archive and Newsflash</title>
      <link>/posts/2017-03-11-rock-climbing-internet-television-news-archive/</link>
      <pubDate>Sat, 11 Mar 2017 01:28:00 -0500</pubDate>
      
      <guid>/posts/2017-03-11-rock-climbing-internet-television-news-archive/</guid>
      <description>Background The Internet Archive’s Television News Archive is a cool way to search closed captions from TV shows.
Here’s a bit more information on it:
 The Internet Archive’s Television News Archive, GDELT’s Television Explorer allows you to keyword search the closed captioning streams of the Archive’s 6 years of American television news and explore macro-level trends in how America’s television news is shaping the conversation around key societal issues.</description>
    </item>
    
    <item>
      <title>Is the flu really worse this year? Comparing the (ongoing) 2016-17 and 2015-16 flu seasons</title>
      <link>/posts/2017-02-28-comparing-cdc-data/</link>
      <pubDate>Tue, 28 Feb 2017 09:28:00 -0500</pubDate>
      
      <guid>/posts/2017-02-28-comparing-cdc-data/</guid>
      <description>Background I was sick last week, and I think I might have had a mild case of the flu. Since it seems like a lot of people have been sick, I was curious whether the flu was really worse this year than last… and since the CDC makes the data available for each year, I put the data together and created a GIF. This project is just a first attempt but the code (it’s in R) is available in a GitHub repository.</description>
    </item>
    
    <item>
      <title>prcr: An R Package for Person-Centered Analysis</title>
      <link>/posts/2017-02-17-introducing-prcr/</link>
      <pubDate>Fri, 17 Feb 2017 16:59:00 -0500</pubDate>
      
      <guid>/posts/2017-02-17-introducing-prcr/</guid>
      <description>I’m excited to share that prcr (0.1.0), an R package for person-centered analysis, is now available on CRAN via install.packages(&amp;quot;prcr&amp;quot;).
Person-centered analyses focus on clusters, or profiles, of observations, and their change over time or differences across factors.
The package is designed to be “low threshold but high ceiling”, in that you can do all of the analysis with one function, create_profiles(df, n_clusters), where dfis a data.frame of the variables to cluster, and n_clusters is the specified number of clusters.</description>
    </item>
    
    <item>
      <title>How much do we spend weekly on Groceries? Figuring out using R and Mint (Updated)</title>
      <link>/posts/2017-01-17-exploring-mint-r/</link>
      <pubDate>Tue, 17 Jan 2017 05:16:00 -0500</pubDate>
      
      <guid>/posts/2017-01-17-exploring-mint-r/</guid>
      <description>We started using Mint to keep track of our spending. One of the best features of Mint is the ability to see past patterns of spending (and to use that information to not spend quite as much on, well, coffee, and for those who know me well, sandwiches from Woody’s Oasis).
I listened to an episode of the Not So Standard Deviations, in which the hosts Peng and Parker discussed using spreadsheets and R to keep track of information for taxes.</description>
    </item>
    
    <item>
      <title>Announcing clustRcompaR v.0.1.0</title>
      <link>/posts/2017-01-07-clustrcompar-0.1.0/</link>
      <pubDate>Sat, 07 Jan 2017 10:16:00 -0500</pubDate>
      
      <guid>/posts/2017-01-07-clustrcompar-0.1.0/</guid>
      <description>Alex Lishinski and I worked on an R package over the last year or so. We are excited that it’s now available on CRAN.
You can install the package using install.packages(&#39;clustRcompaR&#39;) (only needed first time) and load it (more on its two functions below) using library(clustRcompaR).
Here’s a description:
 Provides an interface to perform cluster analysis on a corpus of text. Interfaces to Quanteda to assemble text corpuses easily.</description>
    </item>
    
    <item>
      <title>Presentation on an Introduction to R for Programming and Statistical Analysis in Education</title>
      <link>/posts/20160323-presentation-on-an-introduction-to-r-for-programming-and-statistical-analysis-in-education/</link>
      <pubDate>Wed, 23 Mar 2016 08:10:46 +0000</pubDate>
      
      <guid>/posts/20160323-presentation-on-an-introduction-to-r-for-programming-and-statistical-analysis-in-education/</guid>
      <description> Presentation on an Introduction to R for Programming and Statistical Analysis in Education I had the opportunity to present an Introduction to R for Programming and Statistical Analysis in Education on the invitation of my colleague Mete Akcaoglu at Georgia Southern University’s College of Education. The presentation is available here.
 </description>
    </item>
    
  </channel>
</rss>